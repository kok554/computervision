{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW5L87FUNU0EJLacp0q2jD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kok554/computervision/blob/main/VGG%EA%B3%B5%EB%B6%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "dHSg07tBa6Qp",
        "outputId": "bc263db7-5885-47de-f5c4-58a7395fa443"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NVIDIA의 CUDA 컴파일러 드라이버의 버전을 확인하는 명령어\n",
        "!nvcc -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjvkiF3Ya_ui",
        "outputId": "5fe59952-d9e9-4033-aad7-55315f57a143"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 Package import\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "L0RcNyBda_r_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 출력하는 함수\n",
        "def display_images(image_paths, title, max_images=4):\n",
        "  \"\"\" 지정된 이미지 경로의 이미지를 최대 4개까지 화면에 출력하는 함수 \"\"\"\n",
        "  plt.figure(figsize=(12, 3))\n",
        "  for i, image_path in enumerate(image_paths[:max_images]):\n",
        "    # 이미지 파일을 읽어옴\n",
        "    img = plt.imread(image_path)\n",
        "\n",
        "    # 서브플롯을 생성하여 이미지를 배치\n",
        "    plt.subplot(1, max_images, i + 1) # 1 행 max_images열의 서브플롯 설정\n",
        "    plt.imshow(img)  # 이미지 서브플롯에 표시\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "fkdeN8j7a_pO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 카테고리 목록 정의\n",
        "categories = ['Train crack', 'Train normal', 'Val crack', 'Val normal', 'Test crack', 'Test normal']\n",
        "\n",
        "# 각 카테고리에 대해 반복\n",
        "for category in categories:\n",
        "    # 해당 카테고리에 맞는 이미지 경로를 glob을 사용하여 가져옴\n",
        "    # 경로는 카테고리 이름을 소문자로 변환하고 공백을 '/'로 대체하여 생성\n",
        "    image_paths = glob.glob(f'/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/{category.lower().replace(\" \",\"/\")}/*')\n",
        "\n",
        "    # 이미지 경로를 사용하여 이미지를 표시하는 함수 호출\n",
        "    display_images(image_paths, category)\n",
        "\n",
        "    # 현재 카테고리의 총 이미지 수를 출력\n",
        "    print(f\"{category} 총 이미지 수: {len(image_paths)}\")\n",
        "\n",
        "# 전체 이미지 수를 시각화하기 위한 막대 그래프 생성\n",
        "plt.figure(figsize=(10, 6))\n",
        "# 각 카테고리의 이미지 수를 계산하여 막대 그래프의 높이로 사용\n",
        "plt.bar(categories, [len(glob.glob(f'/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/{category.lower().replace(\" \",\"/\")}/*')) for category in categories])\n",
        "# 그래프 제목 및 축 레이블 설정\n",
        "plt.title('Number of Images per Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Images')\n",
        "# x축 레이블을 45도 회전하여 가독성 향상\n",
        "plt.xticks(rotation=45)\n",
        "# 그래프 표시\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6dLrx_2abJx5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 정의\n",
        "transform = transforms.Compose([\n",
        "    # 이미지를 224 x 224 크기로 조정\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    # 이미지를 텐서로 변환\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # 이미지 정규화: 각 채널의 평균을 0.5로, 표준편차를 0.5로 설정\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# 데이터셋 로드 및 데이터 로더 생성\n",
        "train_dataset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/train', transform=transform)\n",
        "val_dataset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/val', transform=transform)\n"
      ],
      "metadata": {
        "id": "2tRkYUIZbLJ2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋의 인덱스를 생성\n",
        "num_of_train = 34550\n",
        "num_of_val = 330\n",
        "\n",
        "# 훈련 데이터와 검증 데이터의 인덱스 리스트 생성\n",
        "train_indices = list(range(num_of_train))\n",
        "val_indices = list(range(num_of_val))\n",
        "\n",
        "# 200개의 무작위 샘플 선택 (학습 데이터 수가 많아 실습을 진행하기 제한이 있어 설정)\n",
        "np.random.shuffle(train_indices) # 훈련 데이터 인덱스를 무작위로 섞음\n",
        "train_subset_indices = train_indices[:200] # 섞인 인덱스 중 처음 200개\n",
        "\n",
        "np.random.shuffle(val_indices) # 검증 데이터 인덱스를 무작위로 섞음\n",
        "val_subset_indices = val_indices[:200] # 섞인 인덱스 중 처음 200개\n",
        "\n",
        "# 사용자 정의 Sampler 생성\n",
        "train_sampler = SubsetRandomSampler(train_subset_indices)\n",
        "val_sampler = SubsetRandomSampler(val_subset_indices)\n",
        "\n",
        "# DataLoader에 Sampler 지정\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size =5, sampler=train_sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size =5, sampler=val_sampler)"
      ],
      "metadata": {
        "id": "RftJgrFnbMoZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG 모델 로드 및 네트워크 구조 확인\n",
        "net = models.vgg19(pretrained=True)\n",
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn7FnDeBbN2M",
        "outputId": "d262a2b6-de1a-44c5-9ae5-af5914c355cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 92.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 모든 파라미터를 고정\n",
        "for param in net.parameters():\n",
        "  param.requires_grad = False # 각 파라미터의 requires_grad 속성을 False로 설정"
      ],
      "metadata": {
        "id": "fP3P4DBIbPNv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier의 마지막 레이어를 Binary Classification Task에 맞게 교체하고, 이 레이어의 파라미터는 학습 가능하도록 설정\n",
        "net.classifier[6] = nn.Linear(4096, 2) # 마지막 레이어를 2개의 출력 노드를 가진 Linear 레이어로 교체\n",
        "net.classifier[6].requires_grad = True # 새로운 레이어의 파라미터는 학습 가능하도록 설정"
      ],
      "metadata": {
        "id": "3G_HkgqUbQlT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수\n",
        "creiterion = nn.CrossEntropyLoss() # 크로스 엔트로피 손실 함수 정의"
      ],
      "metadata": {
        "id": "hHjBQE7NbRww"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim # PyTorch의 최적화 모듈 임포트\n",
        "\n",
        "def train_model(optimizer_name, net, train_loader, val_loader, criterion, num_epochs=20):\n",
        "  # optimizer 설정\n",
        "  if optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # SGD 최적화기 설정\n",
        "  elif optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001) # Adam 최적화기 설정\n",
        "  elif optimizer_name == 'RAdam':\n",
        "    optimizer = optim.RAdam(net.parameters(), lr=0.001) # RAdam 최적화기 설정\n",
        "  else:\n",
        "    raise ValueError(\"Invalid optimizer name. Choose 'SGD', 'Adam', or 'RAdam'.\")\n",
        "\n",
        "\n",
        "  # 학습/검증 손실과 검증 정확도를 저장할 리스트\n",
        "  train_losses = [] # 학습 손실을 저장할 리스트\n",
        "  val_losses = []  # 검증 손실을 저장할 리스트\n",
        "  val_accuracies = [] # 검증 정확도를 저장할 리스트\n",
        "\n",
        "  for epoch in range(num_epochs): # 지정된 에포크 수만큼 반복\n",
        "    net.train() # 모델을 학습 모드로 설정\n",
        "    running_loss = 0.0 # 현재 에포크의 손실 초기화\n",
        "    for i, data in enumerate(train_loader): # 학습 데이터 로더에서 배치 단위로 데이터 가져오기\n",
        "      inputs, labels = data  # 입력 데이터와 레이블 분리\n",
        "      optimizer.zero_grad() # 기울기 초기화\n",
        "      outputs = net(inputs) # 모델을 통해 예측값 계산\n",
        "      loss = criterion(outputs, labels)  # 손실 계산\n",
        "      loss.backward() # 기울기 계산\n",
        "      optimizer.step() # 파라미터 업데이트\n",
        "      running_loss += loss.item() # 현재 베치의 손실을 누적\n",
        "\n",
        "    # 매 에포크마다 평균 학습 손실 계산\n",
        "    train_loss = running_loss / len(train_loader) # 평균 학습 손실\n",
        "    train_losses.append(train_loss) # 리스트에 추가\n",
        "\n",
        "    # 검증 손실 계산\n",
        "    val_loss = 0.0 # 검증 손실 초기화\n",
        "    net.eval() # 모델을 평가 모드로 설정\n",
        "    correct = 0 # 올바른 예측 수 초기화\n",
        "    total = 0 # 총 에측 수 초기화\n",
        "    with torch.no_grad(): # 기울기 계산을 하지 않도록 설정\n",
        "      for inputs, labels in val_loader: # 검증 데이터 로더에서 데이터 가져오기\n",
        "        outputs = net(inputs) # 모델을 통해 예측값 계산\n",
        "        _, predicted = torch.max(outputs.data, 1) # 예측값 중 최대값의 인덱스 추출\n",
        "        total += labels.size(0) # 총 예측 수 증가\n",
        "        correct += (predicted == labels).sum().item() # 올바른 예측 수 증가\n",
        "        loss = criterion(outputs, labels) # 검증 손실 계산\n",
        "        val_loss += loss.item() # 검증 손실 누적\n",
        "\n",
        "\n",
        "    val_loss /= len(val_loader) # 평균 검증 손실\n",
        "    val_losses.append(val_loss) # 리스트에 추가\n",
        "\n",
        "    val_accuracy = 100 * correct / total # 검증 정확도\n",
        "    val_accuracies.append(val_accuracy) # 리스트에 추가\n",
        "\n",
        "    # 에포크 결과 출력\n",
        "    print(f'[{optimizer_name}] Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "  return train_losses, val_losses, val_accuracies  # 학습 손실, 검증 손실, 검증 정확도 반환"
      ],
      "metadata": {
        "id": "IxomBFwLbTTB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_SGD, val_losses_SGD, val_accuracies_SGD = train_model('SGD', net, train_loader, val_loader, creiterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRh_zwlbbUWU",
        "outputId": "270c6ce4-4ace-4451-c09f-2f674f23c4ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD] Epoch 1, Train Loss: 0.3741, Val Loss: 0.0881, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 2, Train Loss: 0.1880, Val Loss: 0.1379, Val Accuracy: 96.50%\n",
            "[SGD] Epoch 3, Train Loss: 0.1398, Val Loss: 0.0829, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 4, Train Loss: 0.1182, Val Loss: 0.0574, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 5, Train Loss: 0.0920, Val Loss: 0.0918, Val Accuracy: 98.00%\n",
            "[SGD] Epoch 6, Train Loss: 0.0910, Val Loss: 0.0451, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 7, Train Loss: 0.0821, Val Loss: 0.0808, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 8, Train Loss: 0.0684, Val Loss: 0.0589, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 9, Train Loss: 0.0782, Val Loss: 0.0416, Val Accuracy: 99.00%\n",
            "[SGD] Epoch 10, Train Loss: 0.0630, Val Loss: 0.0762, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 11, Train Loss: 0.0528, Val Loss: 0.0575, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 12, Train Loss: 0.0901, Val Loss: 0.0811, Val Accuracy: 98.00%\n",
            "[SGD] Epoch 13, Train Loss: 0.0592, Val Loss: 0.0522, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 14, Train Loss: 0.0637, Val Loss: 0.0523, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 15, Train Loss: 0.0695, Val Loss: 0.0645, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 16, Train Loss: 0.0900, Val Loss: 0.0340, Val Accuracy: 99.00%\n",
            "[SGD] Epoch 17, Train Loss: 0.0348, Val Loss: 0.0524, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 18, Train Loss: 0.0508, Val Loss: 0.0538, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 19, Train Loss: 0.0482, Val Loss: 0.0576, Val Accuracy: 98.50%\n",
            "[SGD] Epoch 20, Train Loss: 0.0358, Val Loss: 0.0629, Val Accuracy: 98.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기화\n",
        "net = models.vgg19(pretrained = True) # 사전 훈련된 VGG19 모델을 불러옴\n",
        "\n",
        "# 모든 파라미터의 requires_grad를 False로 설정하여 학습 중 업데이트되지 않도록 함\n",
        "for param in net.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# VGG19의 분류기 부분에서 마지막 레이어를 수정\n",
        "net.classifier[6] = nn.Linear(4096, 2)\n",
        "\n",
        "# 수정한 마지막 레이어의 requires_grad를 True로 설정하여 학습 중 업데이트되도록 함\n",
        "net.classifier[6].requires_grad = True"
      ],
      "metadata": {
        "id": "v4ps0xqTbWHe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_Adam, val_losses_Adam, val_accuracies_Adam = train_model('Adam', net, train_loader, val_loader, creiterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kV3780TbZFw",
        "outputId": "027679dd-0afc-426b-b9d5-f9786948e651"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam] Epoch 1, Train Loss: 0.3606, Val Loss: 0.1691, Val Accuracy: 97.50%\n",
            "[Adam] Epoch 2, Train Loss: 0.1454, Val Loss: 0.0839, Val Accuracy: 98.50%\n",
            "[Adam] Epoch 3, Train Loss: 0.1121, Val Loss: 0.1138, Val Accuracy: 97.00%\n",
            "[Adam] Epoch 4, Train Loss: 0.0945, Val Loss: 0.1158, Val Accuracy: 97.00%\n",
            "[Adam] Epoch 5, Train Loss: 0.0770, Val Loss: 0.0521, Val Accuracy: 99.00%\n",
            "[Adam] Epoch 6, Train Loss: 0.0847, Val Loss: 0.1099, Val Accuracy: 97.00%\n",
            "[Adam] Epoch 7, Train Loss: 0.0633, Val Loss: 0.0579, Val Accuracy: 98.50%\n",
            "[Adam] Epoch 8, Train Loss: 0.0710, Val Loss: 0.0654, Val Accuracy: 98.50%\n",
            "[Adam] Epoch 9, Train Loss: 0.0486, Val Loss: 0.1136, Val Accuracy: 96.00%\n",
            "[Adam] Epoch 10, Train Loss: 0.0439, Val Loss: 0.0535, Val Accuracy: 99.00%\n",
            "[Adam] Epoch 11, Train Loss: 0.0364, Val Loss: 0.0763, Val Accuracy: 98.00%\n",
            "[Adam] Epoch 12, Train Loss: 0.0383, Val Loss: 0.0500, Val Accuracy: 98.50%\n",
            "[Adam] Epoch 13, Train Loss: 0.0273, Val Loss: 0.0618, Val Accuracy: 98.00%\n",
            "[Adam] Epoch 14, Train Loss: 0.0284, Val Loss: 0.0468, Val Accuracy: 99.00%\n",
            "[Adam] Epoch 15, Train Loss: 0.0428, Val Loss: 0.0709, Val Accuracy: 97.50%\n",
            "[Adam] Epoch 16, Train Loss: 0.0419, Val Loss: 0.0549, Val Accuracy: 99.00%\n",
            "[Adam] Epoch 17, Train Loss: 0.0186, Val Loss: 0.0729, Val Accuracy: 97.50%\n",
            "[Adam] Epoch 18, Train Loss: 0.0530, Val Loss: 0.0292, Val Accuracy: 99.00%\n",
            "[Adam] Epoch 19, Train Loss: 0.0316, Val Loss: 0.0665, Val Accuracy: 98.50%\n",
            "[Adam] Epoch 20, Train Loss: 0.0347, Val Loss: 0.0927, Val Accuracy: 96.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 초기화\n",
        "net = models.vgg19(pretrained = True) # 사전 훈련된 VGG19 모델을 불러옴\n",
        "\n",
        "# 모든 파라미터의 requires_grad를 False로 설정하여 학습 중 업데이트되지 않도록 함\n",
        "for param in net.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# VGG19의 분류기 부분에서 마지막 레이어를 수정\n",
        "net.classifier[6] = nn.Linear(4096, 2)\n",
        "\n",
        "# 수정한 마지막 레이어의 requires_grad를 True로 설정하여 학습 중 업데이트되도록 함\n",
        "net.classifier[6].requires_grad = True"
      ],
      "metadata": {
        "id": "Ru3clBLWbaqv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_RAdam, val_losses_RAdam, val_accuracies_RAdam = train_model('RAdam', net, train_loader, val_loader, creiterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "mUOHamnHbdW7",
        "outputId": "1c000f4f-499c-48e7-e331-c746c24a4e9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'VGG' object has no attribute 'paramters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-805518050d4d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses_RAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses_RAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies_RAdam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RAdam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreiterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-cf6d6a774497>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(optimizer_name, net, train_loader, val_loader, criterion, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adam 최적화기 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RAdam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparamters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# RAdam 최적화기 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid optimizer name. Choose 'SGD', 'Adam', or 'RAdam'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VGG' object has no attribute 'paramters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(train_losses_SGD, label='SGD')\n",
        "plt.plot(train_losses_Adam, label='Adam')\n",
        "plt.plot(train_losses_RAdam, label='RAdam')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(val_losses_SGD, label='SGD')\n",
        "plt.plot(val_losses_Adam, label='Adam')\n",
        "plt.plot(val_losses_RAdam, label='RAdam')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(val_accuracies_SGD, label='SGD')\n",
        "plt.plot(val_accuracies_Adam, label='Adam')\n",
        "plt.plot(val_accuracies_RAdam, label='RAdam')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZuMcYbcibele"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_transform_image(image_path, transform):\n",
        "  # 주어진 경로에서 이미지를 열고 RGB 형식으로 변환\n",
        "  image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "  # 변환을 적용하고 차원을 추가하여 배치 형태로 반환\n",
        "  return transform(image).unsqueeze(0)"
      ],
      "metadata": {
        "id": "x3Ml4dUTbgAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 폴더 경로를 딕셔너리로 정의\n",
        "class_folders = {\n",
        "    'crack': '/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/test/crack',\n",
        "    'normal': '/content/drive/MyDrive/Colab Notebooks/ComputerVision/3/test/normal'\n",
        "}\n",
        "\n",
        "# 시각화를 위한 플롯 설정\n",
        "plt.figure(figsize = (20, 8)) # 플롯의 크기를 설정\n",
        "counter = 1 # 서브플롯 인덱스 초기화\n",
        "\n",
        "# 각 클래스 이름과 해당 폴더 경로에 대해 반복\n",
        "for class_name, folder_path in class_folders.items():\n",
        "  # 폴더 내의 모든 이미지 파일 경로를 가져옴\n",
        "  image_paths = glob.glob(os.path.join(folder_path, '*'))\n",
        "  # 첫 5개의 이미지 경로 선택\n",
        "  selected_paths = image_paths[:5]\n",
        "\n",
        "  # 선택된 이미지 경로에 대해 반복\n",
        "  for image_path in selected_paths:\n",
        "    # 이미지를 로드하고 변환\n",
        "    image = load_and_transform_image(image_path, transform)\n",
        "\n",
        "    # 모델을 평가 모드로 설정\n",
        "    net.eval()\n",
        "    with torch.no_grad(): # 그래디언트 계산 비활성화\n",
        "      outputs = net(image) # 모델에 이미지 입력\n",
        "      _, predicted = torch.max(outputs, 1) # 예측된 클래스 인덱스 추출\n",
        "\n",
        "    # 예측된 클래스 이름 결정\n",
        "    predicted_class = 'crack' if predicted.item() == 0 else 'normal'\n",
        "\n",
        "    # 서브플롯에 이미지와 예측 결과 표시\n",
        "    plt.subplot(2, 5, counter)\n",
        "    plt.imshow(Image.open(image_path)) # 원본 이미지를 표시\n",
        "    plt.title(f'True: {class_name} Predicted: {predicted_class}')\n",
        "    plt.axis('off') # 축 표시 비활성화\n",
        "    counter += 1 # 카운터 증가\n",
        "\n",
        "# 레이아웃 조정 및 플롯 표시\n",
        "plt.tight_layout() # 서브플롯 간의 간격 조정\n",
        "plt.show() # 플롯 출력"
      ],
      "metadata": {
        "id": "qLpHryAsbhB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAOekQFObiRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}