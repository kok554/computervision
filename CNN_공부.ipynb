{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2k+JnrT28W4cmKaLeeBPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kok554/computervision/blob/main/CNN_%EA%B3%B5%EB%B6%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zm8wX77F2Xnp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# CNN 모델 정의\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # Convolutional Layer 1: 입력 채널 1개, 출력 채널 10개, 필터 크기 5x5\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2) # Max Pooling Layer: 2x2\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size =5)  # Convolutional Layer 2: 출력 채널 20개\n",
        "    self.fc1 = nn.Linear(20 * 4 * 4, 50) # Fully Connected Layer: 입력 -> 50차원\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "    self.relu = nn.ReLU() # ReLU 활성화 함수\n",
        "    self.softmax = nn.Softmax(dim=1) # Softmax 활성화 함수\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.pool(self.conv1(x))) # Conv1 -> ReLU -> Pooling\n",
        "    x = self.relu(self.pool(self.conv2(x))) # Conv2 -> ReLU -> Pooling\n",
        "    x = x.view(-1, 20 * 4 * 4) # 텐서를 펼침\n",
        "    x = self.relu(self.fc1(x)) # FC1 -> ReLU\n",
        "    x = self.fc2(x)  # FC2\n",
        "    return self.softmax(x) # Softmax 출력"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# CNN 모델 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(20 * 4 * 4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.pool(self.conv1(x)))\n",
        "        x = self.relu(self.pool(self.conv2(x)))\n",
        "        x = x.view(-1, 20 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# 데이터셋 준비\n",
        "# MNIST 데이터를 불러오고, 텐서로 변환 및 정규화\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                          # 데이터를 텐서로 변환\n",
        "    transforms.Normalize((0.5,), (0.5,))           # 데이터를 평균 0, 표준편차 1로 정규화\n",
        "])\n",
        "\n",
        "# 학습용 및 테스트용 MNIST 데이터셋 다운로드 및 로드\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)  # 학습 데이터\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # 테스트 데이터\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # 학습 데이터 로더\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # 테스트 데이터 로더\n",
        "\n",
        "# 모델, 손실 함수, 옵티마이저 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 사용 가능 여부 확인\n",
        "model = CNN().to(device)                                              # CNN 모델 생성 및 GPU로 이동\n",
        "criterion = nn.CrossEntropyLoss()                                     # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)                 # Adam 옵티마이저 설정\n",
        "\n",
        "# 학습 함수\n",
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):  # 배치 단위로 데이터 가져오기\n",
        "        data, target = data.to(device), target.to(device)      # 데이터와 라벨을 GPU로 이동\n",
        "        optimizer.zero_grad()                                  # 이전의 기울기 초기화\n",
        "        output = model(data)                                   # 모델에 입력 데이터 전달하여 출력 계산\n",
        "        loss = criterion(output, target)                      # 손실 계산\n",
        "        loss.backward()                                        # 역전파로 기울기 계산\n",
        "        optimizer.step()                                       # 옵티마이저로 가중치 갱신\n",
        "        # 100번째 배치마다 학습 상태 출력\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# 테스트 함수\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # 모델을 평가 모드로 설정 (드롭아웃 등 비활성화)\n",
        "    correct = 0  # 정확히 예측한 데이터 수를 저장하는 변수\n",
        "    with torch.no_grad():  # 기울기 계산 비활성화 (속도 향상)\n",
        "        for data, target in test_loader:  # 테스트 데이터 배치 단위로 가져오기\n",
        "            data, target = data.to(device), target.to(device)  # 데이터와 라벨을 GPU로 이동\n",
        "            output = model(data)                              # 모델에 입력 데이터 전달하여 출력 계산\n",
        "            pred = output.argmax(dim=1, keepdim=True)         # 가장 높은 점수를 가진 클래스 예측\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # 정답 개수 누적\n",
        "    # 테스트 결과 출력\n",
        "    print(f'\\nTest set: Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# 학습 및 테스트 반복\n",
        "n_epochs = 5  # 에폭 수\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, device, train_loader, optimizer, criterion)  # 학습 함수 호출\n",
        "    test(model, device, test_loader)                         # 테스트 함수 호출\n",
        "\n",
        "# 모델 저장 디렉토리 생성\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# 학습된 모델 저장\n",
        "torch.save(model.state_dict(), 'models/mnist_cnn_model.pth')  # 모델 가중치 저장\n",
        "print(\"Model saved as 'models/mnist_cnn_model.pth'\")          # 저장 완료 메시지 출력\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXcnT20pCdiw",
        "outputId": "804c33d2-61f7-44cc-a271-d03b7992dce5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301521\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.665880\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.595212\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.634517\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.560084\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.659370\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.482914\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.568281\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.588163\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.539102\n",
            "\n",
            "Test set: Accuracy: 8803/10000 (88%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.625563\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.591961\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.608555\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.606259\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.615196\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.497026\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.588341\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.585342\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.606048\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.612578\n",
            "\n",
            "Test set: Accuracy: 8868/10000 (89%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.555816\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.614038\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.507675\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.580513\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.645682\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.583915\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.529299\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.540615\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.506048\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.484163\n",
            "\n",
            "Test set: Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.475513\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.461931\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.488797\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.472297\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.476638\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.478233\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.471642\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.493604\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.472013\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.461242\n",
            "\n",
            "Test set: Accuracy: 9874/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.477013\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.461327\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.461536\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.504609\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.465464\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.491725\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.501077\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.462991\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.461480\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.478292\n",
            "\n",
            "Test set: Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Model saved as 'models/mnist_cnn_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 데이터셋 변환 함수: MNIST -> 짝수/홀수 레이블\n",
        "# 레이블(0~9)을 짝수(0), 홀수(1)로 변환\n",
        "class EvenOddDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.dataset.targets = self.dataset.targets % 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx]\n",
        "\n",
        "# CNN 모델 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(20 * 4 * 4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.pool(self.conv1(x)))\n",
        "        x = self.relu(self.pool(self.conv2(x)))\n",
        "        x = x.view(-1, 20 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 전이학습용 수정된 CNN 모델 정의\n",
        "class TransferCNN(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(TransferCNN, self).__init__()\n",
        "        self.conv1 = original_model.conv1\n",
        "        self.conv2 = original_model.conv2\n",
        "        self.pool = original_model.pool\n",
        "        self.relu = original_model.relu\n",
        "        self.fc1 = original_model.fc1\n",
        "        self.fc2 = nn.Linear(50, 2)  # 출력 차원을 2로 변경 (짝수/홀수 분류)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.pool(self.conv1(x)))\n",
        "        x = self.relu(self.pool(self.conv2(x)))\n",
        "        x = x.view(-1, 20 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 데이터 변환 및 로더 준비\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 원본 MNIST 데이터셋\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 짝수/홀수 데이터셋 변환\n",
        "train_dataset = EvenOddDataset(train_dataset)\n",
        "test_dataset = EvenOddDataset(test_dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 기존 학습된 CNN 모델 불러오기\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "original_model = CNN().to(device)\n",
        "original_model.load_state_dict(torch.load('models/mnist_cnn_model.pth'))\n",
        "\n",
        "# 전이학습용 수정된 모델 생성\n",
        "transfer_model = TransferCNN(original_model).to(device)\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 함수\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "# 테스트 함수\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "# Case 1: Convolutional Layers Freeze\n",
        "for param in transfer_model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in transfer_model.conv2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Training with Convolutional Layers Frozen...\")\n",
        "for epoch in range(5):\n",
        "    train(transfer_model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(transfer_model, device, test_loader)\n",
        "\n",
        "# Case 2: Fully Connected Layer (fc1) Freeze 추가\n",
        "for param in transfer_model.fc1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 옵티마이저 재정의\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, transfer_model.parameters()), lr=0.001)\n",
        "\n",
        "print(\"Training with Fully Connected Layer Frozen...\")\n",
        "for epoch in range(5):\n",
        "    train(transfer_model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(transfer_model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDZEHFGPH3QR",
        "outputId": "86eb626d-a663-49aa-9046-d23dae282883"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-396368fb2f30>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  original_model.load_state_dict(torch.load('models/mnist_cnn_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Convolutional Layers Frozen...\n",
            "Epoch 1, Batch 0, Loss: 5.256649\n",
            "Epoch 1, Batch 100, Loss: 0.013401\n",
            "Epoch 1, Batch 200, Loss: 0.056751\n",
            "Epoch 1, Batch 300, Loss: 0.024768\n",
            "Epoch 1, Batch 400, Loss: 0.017111\n",
            "Epoch 1, Batch 500, Loss: 0.042289\n",
            "Epoch 1, Batch 600, Loss: 0.007125\n",
            "Epoch 1, Batch 700, Loss: 0.008383\n",
            "Epoch 1, Batch 800, Loss: 0.004868\n",
            "Epoch 1, Batch 900, Loss: 0.017127\n",
            "\n",
            "Test Accuracy: 99.29%\n",
            "\n",
            "Epoch 2, Batch 0, Loss: 0.016011\n",
            "Epoch 2, Batch 100, Loss: 0.128650\n",
            "Epoch 2, Batch 200, Loss: 0.015584\n",
            "Epoch 2, Batch 300, Loss: 0.004562\n",
            "Epoch 2, Batch 400, Loss: 0.000661\n",
            "Epoch 2, Batch 500, Loss: 0.009845\n",
            "Epoch 2, Batch 600, Loss: 0.003260\n",
            "Epoch 2, Batch 700, Loss: 0.014521\n",
            "Epoch 2, Batch 800, Loss: 0.021021\n",
            "Epoch 2, Batch 900, Loss: 0.005634\n",
            "\n",
            "Test Accuracy: 99.46%\n",
            "\n",
            "Epoch 3, Batch 0, Loss: 0.003057\n",
            "Epoch 3, Batch 100, Loss: 0.001484\n",
            "Epoch 3, Batch 200, Loss: 0.004154\n",
            "Epoch 3, Batch 300, Loss: 0.003492\n",
            "Epoch 3, Batch 400, Loss: 0.002544\n",
            "Epoch 3, Batch 500, Loss: 0.060356\n",
            "Epoch 3, Batch 600, Loss: 0.007102\n",
            "Epoch 3, Batch 700, Loss: 0.004869\n",
            "Epoch 3, Batch 800, Loss: 0.021952\n",
            "Epoch 3, Batch 900, Loss: 0.010998\n",
            "\n",
            "Test Accuracy: 99.39%\n",
            "\n",
            "Epoch 4, Batch 0, Loss: 0.001943\n",
            "Epoch 4, Batch 100, Loss: 0.002299\n",
            "Epoch 4, Batch 200, Loss: 0.001624\n",
            "Epoch 4, Batch 300, Loss: 0.013207\n",
            "Epoch 4, Batch 400, Loss: 0.017927\n",
            "Epoch 4, Batch 500, Loss: 0.032313\n",
            "Epoch 4, Batch 600, Loss: 0.019047\n",
            "Epoch 4, Batch 700, Loss: 0.001318\n",
            "Epoch 4, Batch 800, Loss: 0.015972\n",
            "Epoch 4, Batch 900, Loss: 0.004020\n",
            "\n",
            "Test Accuracy: 98.84%\n",
            "\n",
            "Epoch 5, Batch 0, Loss: 0.002126\n",
            "Epoch 5, Batch 100, Loss: 0.006766\n",
            "Epoch 5, Batch 200, Loss: 0.036068\n",
            "Epoch 5, Batch 300, Loss: 0.001883\n",
            "Epoch 5, Batch 400, Loss: 0.013920\n",
            "Epoch 5, Batch 500, Loss: 0.041188\n",
            "Epoch 5, Batch 600, Loss: 0.001581\n",
            "Epoch 5, Batch 700, Loss: 0.002704\n",
            "Epoch 5, Batch 800, Loss: 0.036698\n",
            "Epoch 5, Batch 900, Loss: 0.024250\n",
            "\n",
            "Test Accuracy: 99.48%\n",
            "\n",
            "Training with Fully Connected Layer Frozen...\n",
            "Epoch 1, Batch 0, Loss: 0.011150\n",
            "Epoch 1, Batch 100, Loss: 0.001712\n",
            "Epoch 1, Batch 200, Loss: 0.000707\n",
            "Epoch 1, Batch 300, Loss: 0.015374\n",
            "Epoch 1, Batch 400, Loss: 0.006797\n",
            "Epoch 1, Batch 500, Loss: 0.019880\n",
            "Epoch 1, Batch 600, Loss: 0.011282\n",
            "Epoch 1, Batch 700, Loss: 0.000906\n",
            "Epoch 1, Batch 800, Loss: 0.003101\n",
            "Epoch 1, Batch 900, Loss: 0.001347\n",
            "\n",
            "Test Accuracy: 99.59%\n",
            "\n",
            "Epoch 2, Batch 0, Loss: 0.003169\n",
            "Epoch 2, Batch 100, Loss: 0.000309\n",
            "Epoch 2, Batch 200, Loss: 0.001123\n",
            "Epoch 2, Batch 300, Loss: 0.008281\n",
            "Epoch 2, Batch 400, Loss: 0.000612\n",
            "Epoch 2, Batch 500, Loss: 0.000420\n",
            "Epoch 2, Batch 600, Loss: 0.000474\n",
            "Epoch 2, Batch 700, Loss: 0.004544\n",
            "Epoch 2, Batch 800, Loss: 0.005198\n",
            "Epoch 2, Batch 900, Loss: 0.004239\n",
            "\n",
            "Test Accuracy: 99.56%\n",
            "\n",
            "Epoch 3, Batch 0, Loss: 0.000029\n",
            "Epoch 3, Batch 100, Loss: 0.006540\n",
            "Epoch 3, Batch 200, Loss: 0.024126\n",
            "Epoch 3, Batch 300, Loss: 0.000018\n",
            "Epoch 3, Batch 400, Loss: 0.007347\n",
            "Epoch 3, Batch 500, Loss: 0.000069\n",
            "Epoch 3, Batch 600, Loss: 0.002853\n",
            "Epoch 3, Batch 700, Loss: 0.003463\n",
            "Epoch 3, Batch 800, Loss: 0.000588\n",
            "Epoch 3, Batch 900, Loss: 0.001232\n",
            "\n",
            "Test Accuracy: 99.49%\n",
            "\n",
            "Epoch 4, Batch 0, Loss: 0.000441\n",
            "Epoch 4, Batch 100, Loss: 0.003650\n",
            "Epoch 4, Batch 200, Loss: 0.047029\n",
            "Epoch 4, Batch 300, Loss: 0.000354\n",
            "Epoch 4, Batch 400, Loss: 0.004783\n",
            "Epoch 4, Batch 500, Loss: 0.005272\n",
            "Epoch 4, Batch 600, Loss: 0.001405\n",
            "Epoch 4, Batch 700, Loss: 0.000911\n",
            "Epoch 4, Batch 800, Loss: 0.000177\n",
            "Epoch 4, Batch 900, Loss: 0.000493\n",
            "\n",
            "Test Accuracy: 99.50%\n",
            "\n",
            "Epoch 5, Batch 0, Loss: 0.000038\n",
            "Epoch 5, Batch 100, Loss: 0.001722\n",
            "Epoch 5, Batch 200, Loss: 0.000876\n",
            "Epoch 5, Batch 300, Loss: 0.032374\n",
            "Epoch 5, Batch 400, Loss: 0.021887\n",
            "Epoch 5, Batch 500, Loss: 0.021510\n",
            "Epoch 5, Batch 600, Loss: 0.005775\n",
            "Epoch 5, Batch 700, Loss: 0.023560\n",
            "Epoch 5, Batch 800, Loss: 0.001192\n",
            "Epoch 5, Batch 900, Loss: 0.000876\n",
            "\n",
            "Test Accuracy: 99.51%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTAOhkNk216x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}